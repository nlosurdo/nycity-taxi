{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "60fef6a0-9498-4195-b636-5d5c41d83938",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Experiment Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cc0cb195-2548-472b-84eb-6a2a169947e6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import shap\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pyspark.sql.functions import col\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "from databricks.feature_store import FeatureStoreClient\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from mlflow.tracking import MlflowClient\n",
    "fs = FeatureStoreClient()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "70dbc092-60d7-46b4-b950-2383569b3e28",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load feature store table\n",
    "taxi_data = fs.read_table(\"autoguidovie.nycity_taxi.fs_taxi_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "004c9c9b-2772-4be8-ad74-432588185d4b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Conver pySpark DataFrame to Pandas DataFrame\n",
    "df = taxi_data.toPandas()\n",
    "Y = df['Y']\n",
    "X = df.drop(columns=['Y', 'pr_key'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9270a6c7-dc93-43ff-a559-b30c6939fbe6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# One-hot encoding for categorical features\n",
    "categorical_features = ['vendor_id', 'store_and_fwd_flag', 'payment_type']\n",
    "encoder = OneHotEncoder(sparse=False)\n",
    "X_encoded = pd.DataFrame(\n",
    "    encoder.fit_transform(X[categorical_features]),\n",
    "    columns=encoder.get_feature_names_out(categorical_features)\n",
    ")\n",
    "# Concatenate one-hot encoded columns back to the dataset and drop original categorical columns\n",
    "X = pd.concat([\n",
    "    X.drop(columns=categorical_features),\n",
    "    X_encoded],\n",
    "    axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8277d1c1-537d-4cfc-b0e6-2448ccb44d62",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9d6de289-f9e3-4cd1-ac79-f113150592c2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Split the dataset into 80% train and 20% test\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X_scaled, Y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Start MLflow experiment tracking\n",
    "mlflow.set_experiment(\"/Experiments/nycity-taxi/NyCity Taxi - Base Reference\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a9f7f9ad-5f1c-4c82-a9ee-752203952c2d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "with mlflow.start_run(run_name='DecisionTreeRegressor'):\n",
    "    # Train a regression tree model with default parameters\n",
    "    reg_tree = DecisionTreeRegressor()\n",
    "    reg_tree.fit(X_train, Y_train)\n",
    "\n",
    "    # Test the model on the test set\n",
    "    Y_pred = reg_tree.predict(X_test)\n",
    "\n",
    "    # Evaluate the model\n",
    "    r2 = r2_score(Y_test, Y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(Y_test, Y_pred))\n",
    "    mae = mean_absolute_error(Y_test, Y_pred)\n",
    "\n",
    "    # Log the model and metrics in MLflow\n",
    "    mlflow.log_metric('R2', r2)\n",
    "    mlflow.log_metric('rmse', rmse)\n",
    "    mlflow.log_metric('mae', mae)\n",
    "    mlflow.sklearn.log_model(reg_tree, \"model\")\n",
    "\n",
    "    # Plot feature importance\n",
    "    importance = reg_tree.feature_importances_\n",
    "    features = X.columns\n",
    "    indices = np.argsort(importance)\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.title('Feature Importances')\n",
    "    plt.barh(range(len(indices)), importance[indices], align='center')\n",
    "    plt.yticks(range(len(indices)), [features[i] for i in indices])\n",
    "    plt.xlabel('Relative Importance')\n",
    "    plt.show()\n",
    "\n",
    "    # Log the feature importance plot in MLflow\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.barh(range(len(indices)), importance[indices], align='center')\n",
    "    ax.set_yticks(range(len(indices)))\n",
    "    ax.set_yticklabels([features[i] for i in indices])\n",
    "    ax.set_xlabel('Relative Importance')\n",
    "    ax.set_title('Feature Importances')\n",
    "    mlflow.log_figure(fig, \"feature_importances.png\")\n",
    "mlflow.end_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ec5d6d88-c95d-48af-9c1c-7cc9dbdfc723",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "with mlflow.start_run(run_name='Hypertuned XGBoost'):\n",
    "    xgb = XGBRegressor(objective='reg:squarederror', random_state=42)\n",
    "\n",
    "    # Set up GridSearchCV for hyperparameter tuning\n",
    "    param_distributions = {\n",
    "        'n_estimators': np.arange(100, 301, 100),\n",
    "        'learning_rate': [0.01, 0.1, 0.2],\n",
    "        'max_depth': np.arange(3, 8),\n",
    "        'subsample': [0.8, 1],\n",
    "        'colsample_bytree': [0.8, 1]\n",
    "    }\n",
    "\n",
    "    # Set up RandomizedSearchCV for hyperparameter tuning\n",
    "    random_search = RandomizedSearchCV(estimator=xgb, param_distributions=param_distributions, \n",
    "                                    n_iter=10, scoring='r2', cv=3, verbose=2, random_state=42)\n",
    "\n",
    "\n",
    "    # Train the model with RandomizedSearchCV\n",
    "    random_search.fit(X_train, Y_train)\n",
    "\n",
    "    # Get the best model from RandomizedSearchCV\n",
    "    best_xgb = random_search.best_estimator_\n",
    "\n",
    "    # Make predictions on the test set\n",
    "    Y_pred = best_xgb.predict(X_test)\n",
    "\n",
    "    # Evaluate the model\n",
    "    r2 = r2_score(Y_test, Y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(Y_test, Y_pred))\n",
    "    mae = mean_absolute_error(Y_test, Y_pred)\n",
    "\n",
    "    # Log the model and metrics in MLflow\n",
    "    mlflow.log_metric('R2', r2)\n",
    "    mlflow.log_metric('rmse', rmse)\n",
    "    mlflow.log_metric('mae', mae)\n",
    "\n",
    "    mlflow.sklearn.log_model(best_xgb, \"model\")\n",
    "\n",
    "    # Plot feature importance\n",
    "    importance = best_xgb.feature_importances_\n",
    "    features = X.columns\n",
    "    indices = np.argsort(importance)\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.title('Feature Importances (Best XGBoost)')\n",
    "    plt.barh(range(len(indices)), importance[indices], align='center')\n",
    "    plt.yticks(range(len(indices)), [features[i] for i in indices])\n",
    "    plt.xlabel('Relative Importance')\n",
    "    plt.show()\n",
    "\n",
    "    # Log the feature importance plot in MLflow\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.barh(range(len(indices)), importance[indices], align='center')\n",
    "    ax.set_yticks(range(len(indices)))\n",
    "    ax.set_yticklabels([features[i] for i in indices])\n",
    "    ax.set_xlabel('Relative Importance')\n",
    "    ax.set_title('Feature Importances (Best XGBoost)')\n",
    "    mlflow.log_figure(fig, \"xgboost_feature_importances.png\")\n",
    "\n",
    "    # Run SHAP values\n",
    "    explainer = shap.TreeExplainer(best_xgb)\n",
    "    shap_values = explainer.shap_values(X_test)\n",
    "\n",
    "    # Shap Summary plot\n",
    "    shap.summary_plot(shap_values, X_test, feature_names=X.columns)\n",
    "\n",
    "    # Shap Dependence plots\n",
    "    shap.dependence_plot(\"trip_distance\", shap_values, X_test, feature_names=X.columns)\n",
    "\n",
    "    shap.dependence_plot(\"pickup_day_of_week\", shap_values, X_test, feature_names=X.columns)\n",
    "\n",
    "    shap.dependence_plot(\"pickup_hour\", shap_values, X_test, feature_names=X.columns)\n",
    "\n",
    "    shap.dependence_plot(\"dropoff_clusters_m\", shap_values, X_test, feature_names=X.columns)\n",
    "\n",
    "    shap.dependence_plot(\"pickup_clusters_m\", shap_values, X_test, feature_names=X.columns)\n",
    "mlflow.end_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "577abf08-3664-4376-bf69-1f5441d458ce",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# model_uri = f\"runs:/{run.info.e88a9f75f759457395c3f71af4f17c1c}/best_xgboost_model\"\n",
    "# model_name = \"XGBoostTaxiFareModel\" \n",
    "\n",
    "# registered_model = mlflow.register_model(model_uri, model_name)\n",
    "\n",
    "# client = MlflowClient()\n",
    "# client.transition_model_version_stage(\n",
    "#     name=model_name,\n",
    "#     version=registered_model.version,\n",
    "#     stage=\"Staging\"\n",
    "# )\n",
    "\n",
    "# Load the model artifact from a specific run\n",
    "# run_id = \"e88a9f75f759457395c3f71af4f17c1c\"\n",
    "# model_uri = f\"runs:/{run_id}/best_xgboost_model\"\n",
    "# model = mlflow.sklearn.load_model(model_uri)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [
    {
     "elements": [],
     "globalVars": {},
     "guid": "",
     "layoutOption": {
      "grid": true,
      "stack": true
     },
     "nuid": "742579d4-01be-4d80-9988-e5c1e9469229",
     "origId": 2317119694439731,
     "title": "Untitled",
     "version": "DashboardViewV1",
     "width": 1024
    }
   ],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "nycity-taxi-experiment-ml",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
